<!doctype html><html lang=en><head><meta charset=UTF-8><meta name=description content="completing denton trip
datacenters: gallery wall Alex nodes from the 12/09 visitOverall
CoreWeave is a company built and run by engineers."><title>2025-12-09
</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://azhou35.github.io/my-digital-garden//icon.png><link href=https://azhou35.github.io/my-digital-garden/styles.591589daec716a7d5287f8d56c2c091e.min.css rel=stylesheet><link href=https://azhou35.github.io/my-digital-garden/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://azhou35.github.io/my-digital-garden/js/darkmode.111b0c7178f4cf301c69a167d6c795e4.min.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script><script src=https://unpkg.com/@floating-ui/core@0.7.3></script><script src=https://unpkg.com/@floating-ui/dom@0.5.4></script><script src=https://azhou35.github.io/my-digital-garden/js/popover.37b1455b8f0603154072b9467132c659.min.js></script><script src=https://azhou35.github.io/my-digital-garden/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script><script src=https://azhou35.github.io/my-digital-garden/js/clipboard.25155053855f45fdc48fd92540bcebc9.min.js></script><script>const BASE_URL="https://azhou35.github.io/my-digital-garden/",fetchData=Promise.all([fetch("https://azhou35.github.io/my-digital-garden/indices/linkIndex.3e4d83ca563e91e9f53cebcf3d1c83a8.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://azhou35.github.io/my-digital-garden/indices/contentIndex.6fd8947d8bc239c459d780f771d8d759.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const n=new URL(BASE_URL),s=n.pathname,o=window.location.pathname,i=s==o;addCopyButtons(),addTitleToCodeBlocks();const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=i&&!1;drawGraph("https://azhou35.github.io/my-digital-garden",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2}),initPopover("https://azhou35.github.io/my-digital-garden",!0,!0)},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/azhou35.github.io\/my-digital-garden\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-XYFD95KB4J"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-XYFD95KB4J")}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script><script defer src=https://azhou35.github.io/my-digital-garden/js/search.cf33b507388f3dfd5513a2afcda7af41.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://azhou35.github.io/my-digital-garden/>ðŸª´ anniez</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg>
</label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>2025-12-09</h1><p class=meta>Last updated
Unknown</p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents></nav></details></aside><p>completing denton trip</p><p>datacenters:
gallery wall
Alex nodes from the 12/09 visit<strong>Overall</strong><br>CoreWeave is a company built and run by engineers. There is a lot of thoughtfulness and care put into all aspects of building the datacenters, racks, management software and validation. I would call the latter near-peer, at least.<strong>Manageability and Servicing</strong></p><ul><li>high level, Denton is basically the Fairwater model! CoreWeave is in charge of managing switches (FE, BE, NVLink, management), doing firmware upgrades, and booting the nodes. They have a smart NIC (with its own BMC) that we don&rsquo;t have access to, which they use to set up network routes depending on who currently owns the node. This allows them to do diagnostics / burnin on a particular node while the rest of the rack is handed off</li><li>the main difference from Fairwater is that we have access to the nodes&rsquo; BMCs</li><li>we scared them by saying we&rsquo;d expect to be able to upgrade the firmware on the whole datacenter in 12 hours</li><li>their burn-in suite is quite sophisticated, in some ways even better than ours. They also have a custom gpuburn that loops over different data types, but theirs also does timing / perf. According to them, the GB200 perf variation can differ depending on the data type and our fp32 checks for perf are inadequate. Thankfully this is not hard to fix. (ccÂ 
<a href=https://openai.enterprise.slack.com/team/U07BQ7FDKC2 rel=noopener>@saagarp</a>Â since you were pulled into that workstream)</li><li>they also run full-rack validation, but didn&rsquo;t really articulate how that helps</li><li>they claim to test for and find all the miswires - we should not see any on handed off nodes</li><li>like us, also found the perf variation in GB200s to be absurd and have complained to Nvidia. Unlike us, they are not able to RMA slow chips, but are excited that they&rsquo;ll acquire this ability transitively</li><li>similarly, they monitor FEC bins, but aren&rsquo;t allowed to RMA based on just that. They were very excited we had the power to change this</li><li>they only hand off racks at vd64+</li><li>they also prioritize repairs based on vd64 availability, but they didn&rsquo;t quite understand why they&rsquo;d need our view of the world to make the best decisions</li><li>they have a new triage API we&rsquo;d need to integrate with and a new inventory API.</li><li>they employ 12-14 DCTs per building in shifts, staffed 24/7</li></ul><p><strong>Layout</strong></p><ul><li>each building has ~40 of rows, with each row having 8 racks</li><li>they also have 7 spare warm racks (the last row) in each building they use for spares and triage</li><li>the rows are labeled <building letter>&lt;2 digit row number><rack index>, which is a very nice schema and maps neatly onto ours (we&rsquo;d just replace the letter with the corresponding number)</li><li>they&rsquo;re also adamant about calling them nvlink domains, not racks (just like our scaleup domains)</li><li>they had to live with some awkward asymetry supposedly based on our requirements - 7 of the racks in each row are connected to the T0 rack in the row, but all the last racks are home run to the end of the datacenter where there&rsquo;s a group of T0s for them</li><li>the datahall we visited was pretty dark - the lights in the cold aisles were off. They said it&rsquo;s a panel fault they&rsquo;re fixing</li></ul><p>Safety and Security</p><ul><li>no ID checks until we got to the middle of the datahall, where they set up a temporary screening station.</li><li>security scan thoroughness was on par with Azure&rsquo;s, maybe without checking socks and shoe soles</li><li>had to go through the same scan to exit the area</li><li>phones are allowed in, but photos are not permitted (I took one with an exception)</li><li>ear protection was mandatory and everyone inside was wearing some form of it. They were pretty adamanant about this and mentioned OSHA. Big constrast with OCI.</li><li>signs are in Russian and English (apparently lots of primary Russian speakers among low voltage cablers). see attached - I was a bit shocked.</li></ul><p>Racks and cabling</p><ul><li>the per-rack CDUs were the main distinguishing feature</li><li>they also have temperature/humidity sensors connected to the CDUs low on the rack on the hot aisle side next to the cable catridges - this might help us get the data we were after</li><li>smart power strips inside network racks are not actually configured/connected to a network (no one else seems to bother either)</li><li>cool label jackets (something likeÂ 
<a href=https://www.newegg.com/panduit-nwslc-2y-labels/p/2US-002H-00033 rel=noopener>this</a>)</li><li>pretty neat cabling overall</li><li>they use trunk cables between buildings, but use standard fiber inside a building.</li><li>the T2s are only have the 1 building connected right now and they didn&rsquo;t do any pre-wiring for the rest. I mentioned the concern that they&rsquo;ll be wiring new new buildings while some are already in use, but they&rsquo;re not worried.</li></ul><p>Cooling:</p><ul><li>the datacenter uses chillers and doesn&rsquo;t consume any water (same as OCI)</li><li>in case of outage, chillers stop, but pumps continue to run on UPS. There are large tanks to store cooled down water as a thermal battery.</li><li>CDU runs on a different circuit and from different power shelves than the rest of the rack</li><li>liquid cooling lines use all stainless steel connectors to the CDUs</li><li>they had bladders with glycol hanging off each rack, apparently they&rsquo;re needed as part of bringup to top up the cooling lines</li></ul><p><strong>Interesting bits I gleaned about their nvlink journey</strong></p><ul><li>they swapped 15 to 30% of nodes in early pinto clusters! (ccÂ 
<a href=https://openai.enterprise.slack.com/team/U055X2GKR18 rel=noopener>@reza</a>)</li><li>more non-node issues are resolved by cable cartridge swap than a switch (like 55/45). They&rsquo;ve swapped a lot of cable cartridges (probably the only ones to do so)</li><li>they&rsquo;re moving (seemingly quickly) from smc to dell for odm. Not a total switch but they seem to really prefer dell. The stated benefits are much stronger partnership with dell than smc, and more control over both repairs and provisioning than smc is willing to give them</li><li>personally I thought the data sec screening was a little silly &ndash; full metal detectors in and out and checks in hardhats, but personal phones were allowed in too? I&rsquo;m used to all these things needing to be left outside the secure area entirely from past experience</li><li>the average knowledge level of a cw tech on the floor seemed astronomically high compared to other places I&rsquo;ve seen</li><li>Their provisioning and lifecycle automation is pretty neat (built entirely around a shadow k8s cluster to manage all the resources in the facility) and handles all node/switch/cdu/rack provisioning and validation</li><li>Counter to what we&rsquo;ve seen in our GB200 perf variance CW seems to more reliably measure +/-8%. Still clearly worse than spec but better though.</li></ul></article><hr><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://azhou35.github.io/my-digital-garden/js/graph.afdb02e537635f9a611b53a988e5645b.js></script></div></div><div id=contact_buttons><footer><p>Made by anniez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2026</p><ul><li><a href=https://azhou35.github.io/my-digital-garden/>Home</a></li><li><a href=https://twitter.com/azhou35>Twitter</a></li><li><a href=https://github.com/azhou35>Github</a></li></ul></footer></div></div></body></html>