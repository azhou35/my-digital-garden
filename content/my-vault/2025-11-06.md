---
title: "2025-11-06"
---

- [ ] message Joel 

day 1 
five in one cluster 
unless 

1) for single node maintenances - azure marks all nodes as ua 
	1) for azure steady state maintenance is completely automated
2) for multi-node (aka ib switch) 
	1) we dont wanna ua 32 nodes when only 3 are broken 
	2) if shallotpeat was launching 
3) to get internal names get ndb
	1) backend names here
	2) most of these are research clustesr
	3) more disruptive
	4) research automation is prety good - it wont ofr nodes if we'll switch the nodes
4) interacting w any launches
	1) scheduled on diff days 
	2) late night pacific time 
5) for applied its better to do 
	1) peak 10PM - 5AM 
	2) when Europe is getting 
6) CRD updates for applied here 
	1) 14 hrs 
	2) wanna do crd update 
	3) moving clusters
7) stall indefinitely

oldest gb200 


### Action Item
- get a feel for cluster transfer
- we're moving it for 
- can we afford to do a downtime here 
- get signal on whether this can be rolling 
- get signal for mihai / alexr on how badly we should want fw1.3 
	- phx60 - 2k 
	- f6 is old research cluster being turned
	- c274 is applied
	- kast 
- signal from joel 

#2
c44
c29prdgpc01 / 02 


CW - 


CW - Phoenix - pushed this off 

try to have them in the sheet
sheet is source of truth here
any nodes/switch its fine


fleet-notifier for researcher side 

tag fleet notify 
notify messages
find everyone running jobs in argon

/inc create 
c229 planned-ib-maintenance
hardware 
tg the oncalls and bring theem into issues

/inc close 
some reserve capacity in cluster

![[Screenshot 2025-11-06 at 12.10.38 PM.png]]

this view for fleet dri 
something for sigma
