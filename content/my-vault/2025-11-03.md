---
title: "2025-11-03"
---

Weekly to dos:

SLA 
1) Meet Milestone 1 for 
	1) Track researchers usage of flex compute
	2) Allow other researcher to use flex compute 
2) Determine cluster rollout plan 

Maintenance
1) Runbook for GCP maintenance 
2) AWS meeting 
	1) Check in w Gene beforehand 
3) OCI automation 
4) Pick projects from maintenance consolidation 

P0) 
- 85% hardware health SLA project 

Resources
[[cross-platform maintenances]]
![[Screenshot 2025-11-03 at 10.48.17 AM.png]]
---

how do i get traction on this overallocation problems?
- shantanu's goals:  researcher experience 
- mengqing's goals: developer experience 
- jos' goals: researchers better understand compute and flex 
	- Currently flex usage is ‘first come first serve’, but our intention is to introduce a fairness system that can provide guarantees that every job will run.
- natan's goals:
	- ??? but he has a lot of context 
- leo chen:
	- ok with some buffer but pushes back on 15%

real dependencies:
- data on what gpus we'd haircut in introducing this 
- Currently flex usage is ‘first come first serve’, but our intention is to introduce a fairness system that can provide guarantees that every job will run.

how do we make sure we're not haircutting the fleet by 15%? 
=> goal should not be to reduce by 15% 
i think make small changes and start w clusters that are overallocated 
=> maybe i ne()ed a spreadsheet w numbers on how the compute would be redistributed 
as more goes into flex 
> You're allocated here for 3.0k -- I kept it at ~85% SLA such that when nodes fall out of rotation due to HWH, it doesn't count against your quota.But feel free to schedule on the entire cluster.![[Screenshot 2025-11-03 at 12.33.13 PM.png

how do we get 4113 nodes overallocated down to 2k nodes overallocated?

test with a cluster that is overallocated and moving to flex capacity

