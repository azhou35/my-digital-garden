---
title: "2025-10-28"
---

always allocating up to sla - 
kinds of things that are happening 
- released new health checks that took out a bunch of capacity 
- turned on a health check 
a lot of borderline 
not getting to thinking with any regularity 

allocating up to 85% up to budget 

1243 - 1190 are assigned
95% of what is seen in zoku is assigned

zoku doesnt see the 1280 

1190 

if im in zoku editing quota 
![[Screenshot 2025-10-28 at 10.57.56 AM.png]]
ensure that you cant ensure the current sla of quota 

ndb -> quota 

target: healthy count: 85% of 1280 

if we incrementally increase the sla 
85% isnt always the calculation 
but we have some tweaks
- hide those before any consumer 


Example:
- Capacity: 1237 
	- Healthy: 1072
	- Unhealthy: 165
- Quota:
	- Assigned: 1190
	- Allocated: 1081

### HH Weekly
Eri 
- number of cloud providers has increased complexity of things 
- support reimage 
Quota-Level  

- many clusters, hard to keep at [x]% healthy
- nodes always fall off the bus, many clusters have mismatching priorities with quota (0 to 999) + workloads (crit/high/mid/low)
    - there is no way of prioritizing which nodes should always stay within rotation
    - there is no easy way of determining if we decide to keep a baseline SLA, what that % should be and what to adjust to stay consistent with our PagerDuty

Cluster Level  

- there is no easy way of understanding what is easy to manage for nodes that fall out of rotation:
    - is it one persons job?
    - is it one workstreams workloads?
    - is it everyone's at round-robin?
    - is it a less important job?
- If we cannot bring healthy nodes back because GPUs are in the shop and it takes time to manufacture, ship, and install new GPUs, who do we prioritize?

There are a handful of questions when it comes down to understanding how to fix the `how do I get my full allocation of quota` but I think a question I'd love the answer to is:  

- Is it a quota-guarantee problem?
    - I was told 100
    - I expected 100
    - I need 100

or  

- Is it workload-guarantee problem?
    - I need 100
    - I expected 100
    - Anything less than 100 I cannot use.

[  
](https://app.slack.com/services/B061YELRNBU)

**eri**
maintenance operations
- daniel li - translated it into something that will work on our system
- they will send us a health signal 
- eta by next week 
automated ticket filing
- eri is waiting on list of errors 
	- tony their pm said will be done 
rma validation
- that happens autmatically when we have implemented their nrc client 
	- baremetal validation is done in nrc
- nvc is only for vm providers - l24 
autoscaler
- done - gb200 
	- doesnt work for in zero boot strap clusters
	- f5/f6 
cutsheet
- waiting on oci
- we at least have serials - missing now 
- John Fish 
- no location 

switch telemtry / node telemetry 

oci - 
- on thing
off the plate:
- generally when we have a new cloud provider 
- engineering work 
- communication type stuff 
- scheduling meetings 
- actual implementation 

after that inital appens 
work 
hpe 

OCI - we need the documentation - 
there's 


SimonT 
on oct 31 - we're getting 4k gpus 
line item in a supply table 
what do we accept capacity as
larger batches rather than rack at a time
3k gpus made unavailable 
400 are made uanvaible

when Simon asks what is the scope
shuold match our own supply sheet
make sure every GPu from diff providers

any time we add quota - we effectively lock trget capacity 

c180 - 

Fill out info here 
https://engine-manager-web.engine-manager-0.internal.api.openai.org/clusters/c180 

live cluster there already 
no cluster there - when they get the nodes - they could do some pre work
spin up new k8s cluster

taking applied capacity -> research
are we gonig to backfill w anything
redirecting capacity 

nature of applied 
wont be able to do more moves until we get a couple more in production 
worked w c180 
c180 

Simon will import 

he has imported reservation across the fleet
then he has pivot builder 
= alter the regex formula for new cluster
=> get all the resrvation capacity for the cluster 

this is a hybrid cluster of reservation
if its just quota - only ened to worry about total quota
can see everything that sits within c180

quota solver - move all the engines out of the cluster 
reservations require hand holding 

get all his urls automatically populated

idea:
- have a channel created for c132
- start with api 
- pull in capacity - quickly snapshot it 
work thru actual moves 
have a list of who you should tag operationally
disabled quota 

Opening reservations page in in engine 
manager
filtering by cluster to get all resrervations


top questions with kenny 
---- 

three ways we can think about it
1) first win 
	1) overallocaiton problem
	2) no ones really sat down and hammered thru it 
2) building structure
	1) how do we get all of scheduled maintenance
	2) every day we execute
	3) in decom - we send a report 
	4) some cadence of mainenance 
	5) we get to go out and say and we have to 
	6) is there any maintenance that we need to do 
3) identifying top problems
	1) if ur sourcing problems
	2) generalizing some buckets of rpoblems
	3) out of these 20 that ppl have been talking about
	4) identifying top n problems 
	5) and how to prioritize w saagar
	6) how to prioritize w frameworks like sku sprawl
	7) helps us tell a story of what we should build

quarter
get your first win 
evangelize urself
build the structure
no one picked it up 
and built it structure
identify problems

Sourcing problems
- probably what someone at the next level
- what utilizaiton looks like
how do we uplevel the utilization story
how do we get traction on this 
priority problem
see what helps he needs

perfect thing to do 
what is too operationally toily that i can take off your hands 

short - term 
medium - term
long - term

top n problem 
how do you track against hardware health sla
point at boxes
building the data model 
an intuition on what needs to get 
doing a lot of discovery 

fleet sla 
- not necessarily being acted on 
ppl generally know what automations to build
not acted - a ton of ppl are under water

how is this going to be practically implemented and roled out 
forced to ask 
how does everyone fit in w everyones work 
we can totally hit shantanus goals
implementation 

just trying to manage the impact
how do we make sure ppl dont obsessively complain about compute 

bc we are overallocating 
everything else is in scope


40-45 day mark
December - according to what we want to pplan for 2026 
cool to take on amd deployment 
