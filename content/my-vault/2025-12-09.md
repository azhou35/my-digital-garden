---
title: "2025-12-09"
---

completing denton trip

datacenters:
gallery wall
Alex nodes from the 12/09 visit**Overall**  
CoreWeave is a company built and run by engineers. There is a lot of thoughtfulness and care put into all aspects of building the datacenters, racks, management software and validation. I would call the latter near-peer, at least.**Manageability and Servicing**  

- high level, Denton is basically the Fairwater model! CoreWeave is in charge of managing switches (FE, BE, NVLink, management), doing firmware upgrades, and booting the nodes. They have a smart NIC (with its own BMC) that we don't have access to, which they use to set up network routes depending on who currently owns the node. This allows them to do diagnostics / burnin on a particular node while the rest of the rack is handed off
- the main difference from Fairwater is that we have access to the nodes' BMCs
- we scared them by saying we'd expect to be able to upgrade the firmware on the whole datacenter in 12 hours
- their burn-in suite is quite sophisticated, in some ways even better than ours. They also have a custom gpuburn that loops over different data types, but theirs also does timing / perf. According to them, the GB200 perf variation can differ depending on the data type and our fp32 checks for perf are inadequate. Thankfully this is not hard to fix. (cc [@saagarp](https://openai.enterprise.slack.com/team/U07BQ7FDKC2) since you were pulled into that workstream)
- they also run full-rack validation, but didn't really articulate how that helps
- they claim to test for and find all the miswires - we should not see any on handed off nodes
- like us, also found the perf variation in GB200s to be absurd and have complained to Nvidia. Unlike us, they are not able to RMA slow chips, but are excited that they'll acquire this ability transitively
- similarly, they monitor FEC bins, but aren't allowed to RMA based on just that. They were very excited we had the power to change this
- they only hand off racks at vd64+
- they also prioritize repairs based on vd64 availability, but they didn't quite understand why they'd need our view of the world to make the best decisions
- they have a new triage API we'd need to integrate with and a new inventory API.
- they employ 12-14 DCTs per building in shifts, staffed 24/7

**Layout**  

- each building has ~40 of rows, with each row having 8 racks
- they also have 7 spare warm racks (the last row) in each building they use for spares and triage
- the rows are labeled <building letter><2 digit row number><rack index>, which is a very nice schema and maps neatly onto ours (we'd just replace the letter with the corresponding number)
- they're also adamant about calling them nvlink domains, not racks (just like our scaleup domains)
- they had to live with some awkward asymetry supposedly based on our requirements - 7 of the racks in each row are connected to the T0 rack in the row, but all the last racks are home run to the end of the datacenter where there's a group of T0s for them
- the datahall we visited was pretty dark - the lights in the cold aisles were off. They said it's a panel fault they're fixing

Safety and Security  

- no ID checks until we got to the middle of the datahall, where they set up a temporary screening station.
- security scan thoroughness was on par with Azure's, maybe without checking socks and shoe soles
- had to go through the same scan to exit the area
- phones are allowed in, but photos are not permitted (I took one with an exception)
- ear protection was mandatory and everyone inside was wearing some form of it. They were pretty adamanant about this and mentioned OSHA. Big constrast with OCI.
- signs are in Russian and English (apparently lots of primary Russian speakers among low voltage cablers). see attached - I was a bit shocked.

Racks and cabling  

- the per-rack CDUs were the main distinguishing feature
- they also have temperature/humidity sensors connected to the CDUs low on the rack on the hot aisle side next to the cable catridges - this might help us get the data we were after
- smart power strips inside network racks are not actually configured/connected to a network (no one else seems to bother either)
- cool label jackets (something like [this](https://www.newegg.com/panduit-nwslc-2y-labels/p/2US-002H-00033))
- pretty neat cabling overall
- they use trunk cables between buildings, but use standard fiber inside a building.
- the T2s are only have the 1 building connected right now and they didn't do any pre-wiring for the rest. I mentioned the concern that they'll be wiring new new buildings while some are already in use, but they're not worried.

Cooling:  

- the datacenter uses chillers and doesn't consume any water (same as OCI)
- in case of outage, chillers stop, but pumps continue to run on UPS. There are large tanks to store cooled down water as a thermal battery.
- CDU runs on a different circuit and from different power shelves than the rest of the rack
- liquid cooling lines use all stainless steel connectors to the CDUs
- they had bladders with glycol hanging off each rack, apparently they're needed as part of bringup to top up the cooling lines

**Interesting bits I gleaned about their nvlink journey**  

- they swapped 15 to 30% of nodes in early pinto clusters! (cc [@reza](https://openai.enterprise.slack.com/team/U055X2GKR18))
- more non-node issues are resolved by cable cartridge swap than a switch (like 55/45). They've swapped a lot of cable cartridges (probably the only ones to do so)
- they're moving (seemingly quickly) from smc to dell for odm. Not a total switch but they seem to really prefer dell. The stated benefits are much stronger partnership with dell than smc, and more control over both repairs and provisioning than smc is willing to give them
- personally I thought the data sec screening was a little silly -- full metal detectors in and out and checks in hardhats, but personal phones were allowed in too? I'm used to all these things needing to be left outside the secure area entirely from past experience
- the average knowledge level of a cw tech on the floor seemed astronomically high compared to other places I've seen
- Their provisioning and lifecycle automation is pretty neat (built entirely around a shadow k8s cluster to manage all the resources in the facility) and handles all node/switch/cdu/rack provisioning and validation
- Counter to what we've seen in our GB200 perf variance CW seems to more reliably measure +/-8%. Still clearly worse than spec but better though.