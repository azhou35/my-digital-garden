---
title: "2025-10-23"
---


to do:
- [x] prep for saagar
- [x] maintenance story
	- [ ] do a tracker for oci / gcp maintenance
	- [ ] https://docs.google.com/spreadsheets/d/1FMsxbeGtLAQnE9OGbottub6-hqcx6CLxMA3MohSsdCM/edit?resourcekey=&gid=818506590#gid=818506590 
	- [ ] cw https://openai-corpws.slack.com/archives/C08R6GA4MBR/p1760544055042179
	- [ ] google:
		- [ ] Thu, Dec 18, 2025 at ~12p
		- [ ] Sep day: asia-northeast1-b maintenance
- [ ] reach out to folks 
	- [ ] general
		- [ ] ikai 
	- [ ] resource overallocation
		- [ ] anurag reddy
		- [ ] leo chen
	- [ ] saagars team
		- [ ] eri schwartz
	- [ ] 
	
End of Q4 is:  
- The overallocation problem is solved
- We have line-of-sight for minimum 3-6 months on what maintenances we need to do and prioritize internally
- We have line-of-sight and a great process for being able to ingest maint. events from all our vendors.
    - + a plan to make this better over time so you don't have to reconcile it all the time.
Cluster

- ![[Screenshot 2025-10-23 at 2.05.43 PM.png]]
- VM models for these assignments 
- reservations most important to be deleted

Saagar - 
share some of my goals
learn about some of the tools that ur using to track
learn more context about hardware health and overallocation 
-> got a bit lost by 


https://docs.google.com/spreadsheets/d/13pEuYYnGX_xkyqjP4Gk6EDQ9UGA0YZEbjKX2ABtDZJc/edit?gid=1469705073#gid=1469705073
- sends an automation to engine manager to decomission it officially 
https://docs.google.com/spreadsheets/d/1F9jF9Zr1U3beAF_is6M_eZP-sW4R6WmSBwYDGs8ADOM/edit?gid=1521527523#gid=1521527523
![[Screenshot 2025-10-23 at 2.29.04 PM.png]]![[Screenshot 2025-10-23 at 2.29.45 PM.png]]

- have align decomming 1 thing from applied w transferring from research
	- if transfer # of 9k GPUs from applied need 9k GPUs for research 
- wont do pinning in the future 
- node database
	- https://go/nbd
- engine manager - cluster info for applied
- nbd - cluster info for research
- zoku - manages quota side of things 
- go from physical cluster -> ndb to convert into research cluster -> what is used by that is defined by quota 
	- on zoku - interact w anurag to kick ppl off 
- what we prioritize - 
	- eg one of these clusters is primary to search-us-ib-320
	- if we kick off search before we give them the cluster 
	- lets not do c184 first
- none of these will be interestigng
- pick one to get moving 
- c202 

how we map all the hosts 
- gb200s - 72 racks
- nvl has 72 
- each every gpu ni the rack
- IMEX domain - how do we resolve all the hosts
	- nvl72 domain 
- ppl across diff providers manage it differently 
	- our preference: static 
		- 1) want to know what is deterministic when we bring in node 
			- programmatically bring in nodes
			- know that 0.1 is within rack #10 
		-  elevation - want to know where in the rack it lands
		- 2) when we cycle nodes out for rma - want to know where the gpu left and where to bring it back - know exactly where to bring it back
	- ^ inventory api (for oci) / cut sheets (from ppl like coreweave)
	- applies to baremetal
	- when we do vms - pulls gpus from anywhere 
		- makes this very hard
		- trying to pass off a lot of requirements to gcp / azure for **rack level awareness**
	- why is this important?
		- if we put the last rack out of commission - we can pull from another rack 
			- if we bring a node back it may not go back in the same domain 
		- how can you give us some primitives to manage things on a rack level 
		- each capacity block is towards a rack
	- capacity block #1 / capacity block #2 
	- prev for h100/h200 we dont care about what it is 
		- now we want to scale everything - bound by rack 
- we care about elevation 
- and determinism 
	- elevation is synonymous and determinism 
	- we want to know what elevation
- care about whether 
	- if you give us gpu over there it doest help
- if we bring it back as full of a domain as possible 
- if u give gpus across 10 gpus across 10 racks 

questions 
- ask how do you fill up racks 
- rmas - how do you prioritize this
	- how do you manage things on a diff level 
	- implementation of these things 
- have a convo and understand on why things are things
- if ur not tracking rack and elevation 
- give them the heuristics sort of thing
- tell me how ur doing it and 

GPU lifecycle: In order to maximize available GPUs, we have to manage a very complex state machine with tons of SKUs and cloud providers to handle repairs and upgrades