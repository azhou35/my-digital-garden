---
title: "2025-12-10"
---

noticed 15 links down related to 1 switch
12 hrs later noticed switch itself went down 

multiple switches on the link went bad
soft reboot -> hard reboot 
cpld upgrade requires a lot of downtime and downtime

more interesting problems

main goal

sreekanth/uday 
- vision of a better future 
	- where some of the day2day operations 
	- find some pppl to handle day to day ops 
	- empower to do this toilsome by hand today
	- figure out what are the tools 
- tpms on frontier side
	- sam truslow
	- tpm have been driving frontier deployments
	- fleet + frontier team
- operational load 
csp stuff is pretty light
variants of new things is pretty light
we dont have a machanism for switch validation/rack validation
- until we have this we're just human gluig 
- how do we get ahead of the mess coming later in the year

going to spend a lot of our time on system architecutre type things 
to make it easier to onboard telemetry from diff providers in diff format
long tail work of this is the set of things and how will we get it for u
what is do we need to get here
strategize w reza on this 
core problem is to see what 
we can start to have this convo nonurgently
mostly important for research clusters
- we're not using that 


defining their requirements 
communicating the csps 
superset and seeing where are we 
prioritize internally 

1 pge summary of here are things that are important to us 
and use cases of whats important to this 

find time to touch base skreeahnth 
this week

we're doing these argon repaves 3x a week - we do it ourselves and its healthy
in azure - we scale down and wait 
set of operational tools that can execute these, track how we're doing 
space to define what they look like
what they need to support
how we build an operational group around using those things
defining the requirements for these tools
how do we get them staffed

how much control 

tons of room for improvement 

azue has heir firmware targetting api 
in an ideal world more direct api integrations
giving HPE operational tools whenever we could 
the cases when we ned to run the whole thing 
1p datacenters
org support 
go/next

pre-work - wenlong should be doing it 
to make sure everything is good and ready to go

we hsould have a dashboard of this is when nodes came back into production 
this

very shortly before we scale the nodes out 
and 
konw we how validation can take
should know how long validation should finish 
graph get built of heres the expected nodes of 
python shell that can run a maintenance
turn that into a workflow and expand it various vendors 
conceptually the ability to do it 
have somoene page somebody

it's a bit fragile, we can use it today for GB200 with few assumptions, the GPU UUID won't be changed across repair, and it's always 4 GPUs per GPU node, which is different for different SKUs, for GB200/GB300, the hash solution works, but I'm not sure it's future proof

it's better to have a dedicated stable ID that has vendor blessing, so that we avoid any computation from our side

get all rewuirements and blast it 

parity 
saagar's going out soon 

c274
c369