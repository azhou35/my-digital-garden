---
title: "2025-10-25"
---

**
1. Consider other important metrics we should track and build dashboards for them    

2. Amount of time nodes spend in validation
    
3. Round-trip time for OFR for all vendors
    
4. Performance on various validation tests across hardware specs

**

today i fly to vegas w n
bbao

peashooters 
engines
related question: within a single quota, do we have a way to prioritize training jobs over the engines? that is, if _something_ is going to get descheduled, a hint to suggest importance of the job to perhonen? (maybe [@leochen](https://openai.enterprise.slack.com/team/U06FYM1Q423) can speak well to this) (edited)

here. Meanwhile, we are working on a new workload api which will allow users to use critical priority for different pools and specify relative priority among them

We've been following the practice of putting everything as `critical`, including our grader engines. We have done this just to make sure our grader engines don't get preempted by someone else spinning something `critical` in the cluster (maybe inadvertently). But `high` protocol for graders makes sense and would help us here, but this allows other people to kick out some of our grader compute I guess. A relative priority within `critical` would be awesome!

elasticity
allocation
util