---
title: "2025-10-31"
---

> quantify how many gpus would actually be "cut" across the fleet (aka how many clusters are >85% and what %) - so the "haircut" feels less scary

Yeah this is the right approach [holistically](https://openai.slack.com/archives/D09MD8VLL6P/p1761686892532109?thread_ts=1761615214.901449&cid=D09MD8VLL6P)

> re-litigate what acceptable "buffer" is - see if needs to be uniform or if it can be something that's lower for healthier clusters. or see if we can start with a smaller buffer

We can either:  

- Move buffer from 5% -> 10% -> target 15%
    - This is within our control to manage expectations
    - This is slightly easier because it's a people and scheduling problem
- Move HWH SLA from 85% -> 90% -> target [X]%
    - We should figure out what it takes for Hardware to move the needle here
    - This is a lot harder

> better qualify the problem statement here - scheduling team sees this as a hardware trying to reduce tickets vs researchers losing usable compute. better justify why leadership is vested in appropriate quota allocation

This can actually a long(er) term issue of:  

- How do we ensure stable capacity for long(er) so jobs don't die and expectations are managed
    - Less GPUs but **stable/healthy** GPU hours for longer vs.
    - More GPUs but **not stable/healthy** GPU hours for shorter
- How do we give ourselves more opportunities to pool our fleet
    - e.g. If I am moving 200 gpus from training to reasoning, are they being used for the same thing? Why can't we make this general purpose?
    - Relevant [Doc](https://docs.google.com/document/d/1kWiVrgogolZTu0E2mkGQqVPPNRd8L7Ri9mLASuAFNYw/edit?tab=t.0#heading=h.lhdomwybtyac)

My vibe here is no one is opinionated here so it's hard to move the needle or make any progress. We're starting with a small(er) problem statement intentionally because if we start with a big one then people get pedantic, there's a lot of opinions, and no progress ever really gets made bc scared.

> walk: this is where the phased approach is rlly crucial here, since we also want users to feel that the gpus going to flex are actually usable (jos). it wont be "15% gpu cut across the fleet" all at once, and if we start with some easier clusters (underallocated) we can get some feedback

Yes agreed - I trust you to take a conservative, pragmatic approach anyways.



reflections: upon push back how do i move forward? 
what is the decision point to be made here? 


Maybe for really healthy clusters:
- Buffer is average health - a switch going out (32 nodes)

we took over jan 
- All allocations would be in a spreadsheet 
	- no consdieration for cluster size
- if all these teams are expected to have this much compute
- give ppl set compute
	- rest should be flex 
- one chance to reprioritize the entire fleet
ppl dont actually treat compute at flex

researchers are seeing that ur taking compute 
i had 10k on this piece of paper 

when you change the entire process u change 
how do we make this a technical problem

what does usable mean
- 
guarantee of will my job schedule problem 

move these shuffles 
200 gpus into workstream from one to the next

applied fleet 
- elastic compute
- under utilized compute 
how do we pool things so we can genericize the fleet 

we had 0% flex capacity that we can use for anyhting
we can use 10k gpus we can use for anyhting 


given how ur not in this cluster how would use it

who should get priority here
how should see priority
![[Screenshot 2025-10-31 at 3.24.59 PM.png]]


cluster is both unhealthy + overallocated 
