---
title: "2025-10-27"
---

Weekly goals:

#1) Maintenance across providers
*Figure out the toil of automations across our CSPs*

-  [ ]  Talk to hardware health members about each csp (eri/gene/vansh)
- [ ] Create document with plan

Open questions:
- what gaps currently exist in documentation? 

Ideal view should:

verifying scope of process swap
![[Screenshot 2025-10-27 at 2.14.18 PM.png]]

as tpms can
input tables
https://docs.google.com/spreadsheets/d/1F2tVkQit753hwW9ZGw7QRZLK87z8N1bC9rOdhanP6l4/edit?gid=0#gid=0
![[Screenshot 2025-10-27 at 2.21.55 PM.png]]

if you use filters - try to use controls - otherwise u just publish it 
able to use it 

mode - sla 

create a new table - 
find the right the connection
- <1 day work
- assuming access to tables
- read / write perms 

![[Screenshot 2025-10-27 at 3.16.52 PM.png]]

scale / 224 vms
once we hit the 19th 
it will take lall of them
reservation level

if we have maintenance


per-rack basis 
for applied - per-rack basis 
we can updates for 

we have some web ui who can come in and i wanna do maintenance on these clusters 
is it full cluster or rolling
what are the things that will be different 
what is gcp or azure like 
eng team doesnt 
if we can get to a world where we can hire ops team

one afternoon just fire them off 
on gcp side our stuff should auto scale on its own 

they have some implementation 
can vlaidate on these smaller blocks

****

1) want to cordon the nodes that need to be maintenance
2) 1) grab a smallish block 
3) do the next one 

at what cadence its the whole cluster
pick a block and see which nodes are in them 

- touch base with simon / kenny on cluster253 
	- touch base on timing 
- noon 

fabric merge in falun 
ended up being slightly rushed in a decision to proceed
miscommunication
how do we plan these at times that arent terrible

- azure delivered the cluster on the fabric
	- 6k gpus in c180 
	- 6k gpus in research 
	- new gpus
	- intent was to merge
- problem
	- intended to ut new ones in research 
	- now we're taking applied gpus to research 
	- capacity crunch 
	- chat is strved for capacity
- 9-10 
- things will happen and we'll be back 
- started doing merge at 9am 
- 6:30 
- azure 
- didnt technically need to drain 
	- cordon 
- capacity 
- 