---
title: "2025-12-01"
---

things to do 

oom - 
if application ooming 
hypervisor system ooming 
- not much to do excepet cloud provider

hardware health 
[https://grafana.sci.openai.org/d/eexoanvjmmwhsa/ev3-dash-chronosphere?orgId=1&var-cluste[â€¦]terval=30s&var-rank=.%2A&var-pipereplica_group_regex=.%2A](https://grafana.sci.openai.org/d/eexoanvjmmwhsa/ev3-dash-chronosphere?orgId=1&var-cluster=titanium&from=now-7d&refresh=1m&var-namespace=codex&var-exported_namespace=codex&var-rapid_id=.%2A&var-pod=tyranitar-run-rollout-worker-.%2A&to=now&theme=light&timezone=America%2FLos_Angeles&var-datasource=chronosphere&var-interval=30s&var-rank=.%2A&var-pipereplica_group_regex=.%2A) 


f16 - may require a repave


Simon - 
we fully focused in decoms aorund azure clusters
find anyone who worked on caas 
V100 

search google
v100 

carolina contracted v100 ids

concern is if theyre using productionuse case 
replacement capacity 

best case - 
this is the sub id that is impacted - caas folks 
https://www.notion.so/openai/Applied-Capacity-Planning-Group-Quota-CLI-19e8e50b62b080eaa499c98817418871 
https://command-center-web.reliability-1.internal.api.openai.org/capacity-manager/request

massive cluster needs to move a lot of capacity 
can ends up in 50 diff clusters
things are getting in diff 50 diff clusters
changes for improving
if its slow down model weights, slow to pull down engines, spread out across 
a lot of 
product side - they need capacity for serving products
they're getting alerts that theyre getting engines not being scheduled on time

take a subset of these clusters at this side
do we need not build it as large in general

gcp do you need to take out the entire thing at once
wheree we might get a physical domain information
but if we dont have thia in our node database
within c253 i wanna drain this domain 
row of racks 
job gets 

available on provider side and pull it into our system

what is gogoles failure domain


the cluster 
mahmoud - 
- we started the cluster evac 
- 9pm - hit the bottom - bc its a big cluser it puts a lot of stress
	- 10:20 pm - ppl spinning new experiments for chat
- stayed up until 2 until you could make their engines spin up and take priority 
maintenance happened 
- took slightly longer
- maintennce didnt finish correclty
- problem was overlooked
	- daniel mitigated by removed the controller that bans nodes
- moving back cluster things broke
	- tensorapi broke
	- when we moved everywehre into the cluster
	- a lot of pressure going into one place
- egaged the tensor api - working until 10 - 11 pm 
- view to show all the unallocated capacity
	- free - means didnt allocated
	- negative - you allocated capacity - but u promised 
- over the weekend 
	- sujith recycled thru the nodes 
	- tensor api broke again 
- cycled thru more nodes - sora lost capacity
- needed to fail over 
- can edit the quota
	- aaas long as they have capacity here you can move them somewhere 
	- only did this for sora - some ppl who were outside of c253 had to move to c253
tldr 
c253

up to 5k A100es

37420 A100e
c253

in a very ideal scenario can failover

up to 55k
try not to allocated more than 10% in the same quota

need to wait 

scheduling pov - island - set of nodes 
pin to a particular island 
partition these big clusters - based on these blocks 
based into islands 


ak c263 ge ibd -oyaml 
what if we had many of these 
roce block 1 2 3 



### daniel
when we experimented on the block - we found the drain very slow
daniel increased the parallelism 
need some balance the sped and health 
previously during the experiment 
block by block

dont need to wait for block to go back 
stagger it out by 20 minutes 
- add more time at the beginning for gradual drain 
	- we're running into limits for our nodes 
	- if you send the whole cluser 
- what is the maximum number of nodes

during the drain we didnt terminate the pod properly 
networking is overloaded 
it takes 
- maintenance takes seeveral hours - but pod takes several hours 
- tries to terminate the pod 

#1) - gradual maintenance - 6 hour window - 2 hr drain block by block 
#2) move their experiment to other clusters 
	do one block and wait 4 hrs 

we can get info from the blocks
351 011 792 1106