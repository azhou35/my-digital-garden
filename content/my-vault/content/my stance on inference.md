---
title: "my stance on inference"
---

having worked on AI infra at Microsoft supporting the worlds largest AI workloads, what I see is a disconnect in enterprise investment in enabling training vs inference. The biggest bottleneck in inference is things like memory constraints/memory bandwidth limitation/attention mechanism efficiency
improper for inference bc we repurpose the same training infra, same h100s and h200s for inference
GB200s are perfect for inference but 

1. The way I see it, Etched has the right combo of science, engineering in order to create the perfect product for inference - what it needs is highly technically, high agency operators (TPMs and the like)

having worked on AI infra at Microsoft supporting the worlds largest AI workloads, what I see is a disconnect between how quickly LLMs scale vs the size and quality of datasets. 
i have demonstrated interest in data engineering build grounds-up for the scale of LLMs - 