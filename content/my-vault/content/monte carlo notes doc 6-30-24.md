---
title: "monte carlo notes doc 6-30-24"
---

[Data Observability w/ Monte Carlo | by Sam Crowder | Medium](https://samcrowder.medium.com/data-observability-w-monte-carlo-4ee6a6891d89)

In 2006, British mathematician Clive Humby coined the phrase “data is the new oil”, which quickly became a common saying in the corporate world. A more recent sub-trend is that analysts, engineers, CEOs, and others are demanding data with lower latency, so they can respond to new information more quickly and stay ahead of the competition.

Companies previously used data for more macro purposes than the purposes of today. For example, retail businesses may collect quarterly stats about shopper demographics, consolidate reports to be presented at executive meetings, and then with it they may build the company marketing plan for the next six months.

Long-cycle, offline data “pipelines” such as this are in many cases being replaced by shorter-cycle, online pipelines with tighter feedback loops. In a past job, I worked on a pipeline that looked like this:

[Monte Carlo Bets on the Future of Data Observability (ventanaresearch.com)](https://blog.ventanaresearch.com/monte-carlo-bets-on-the-future-of-data-observability)

[Monte Carlo](https://www.montecarlodata.com/) was founded in 2019 by CEO Barr Moses and Chief Technology Officer Lior Gavish, who were previously VP of customer operations at Gainsight and SVP of engineering at Barracuda, respectively. In those![Ventana_Research_Analytics_and_Data_Benchmark_Research_Most_Time_in_Analytics_Process_20221031 (1)](https://blog.ventanaresearch.com/hs-fs/hubfs/Ventana_Research_Analytics_and_Data_Benchmark_Research_Most_Time_in_Analytics_Process_20221031%20(1).png?width=300&height=300&name=Ventana_Research_Analytics_and_Data_Benchmark_Research_Most_Time_in_Analytics_Process_20221031%20(1).png) roles, both had witnessed that while tools and platforms were readily available to enable IT engineers to identify and resolve software and infrastructure failures and performance problems, there was a paucity of offerings available to data engineers to monitor the validity of data pipelines. The founders were inspired by the observability platforms that provide software and infrastructure engineers with an environment for monitoring metrics, traces and logs to track application and infrastructure performance and set out to create a similar environment for monitoring the quality and reliability of data used for analytics and governance projects. Unlike existing data-quality software, which typically provides users with an environment to manually check and correct data-quality issues, data observability software is designed to automate the monitoring of data used for analytics projects. In addition to improving trust in data, this has the potential to reduce time to insight. Almost two-thirds of participants (64%) in our [Analytics and Data Benchmark Research](https://www.ventanaresearch.com/benchmark/analytics/analytics_and_data) cited reviewing data for quality issues as being the most time-consuming aspect of analytics initiatives, second only to preparing data for analysis.


 I assert that through 2025, data observability will continue to be a priority for the evolution of DataOps ![VR_2022_Data_Operations_Assertion_4_Square (3)](https://blog.ventanaresearch.com/hs-fs/hubfs/VR_2022_Data_Operations_Assertion_4_Square%20(3).png?width=300&height=300&name=VR_2022_Data_Operations_Assertion_4_Square%20(3).png)products as vendors deliver more automated approaches to data engineering and improving trust in enterprise data.
These factors are likely to become increasingly important to businesses as data volumes continue to grow and they become increasingly reliant on DataOps and the [orchestration of data pipelines](https://mattaslett.ventanaresearch.com/orchestrating-data-pipelines-facilitates-data-driven-analytics) to support data-driven decision-making.

[Data Observability is Key to Ensuring Healthy Data Pipelines (ventanaresearch.com)](https://mattaslett.ventanaresearch.com/data-observability-is-key-to-ensuring-healthy-data-pipelines)
"cambrian explosion in data"

explanation of data observability: data observability is concerned with the reliability and health of the overall data environment. Data observability tools monitor not just the data in an individual environment for a specific purpose at a given point in time, but also the associated upstream and downstream data pipelines. By doing so, data observability software ensures that data is available and up to date, avoiding downtime caused by lost or inaccurate data due to schema changes, system failures or broken data pipelines. While data quality software is designed to help users identify and resolve problems related to the validity of the data itself, data observability software is designed to automate the detection and identification of the causes of data quality problems. As such, data observability can potentially enable users to prevent data quality issues before they occur.
