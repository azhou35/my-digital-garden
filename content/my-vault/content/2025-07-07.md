---
title: "2025-07-07"
---

goals:
- finish rabbitholeathon
- send some emails to potential ppl at harvey
	- arize
	- 

job:
- follow up w saagar 
- set up call w sagar
- follow up w hpc ops team

jobs:
traversal

Traversal email:
it's a pain point many PMs and engineers have to face
Working on the operations for the supercomputing that runs training ML workloads for OAI, I acutely feel the pain of sitting on bridges, combing through tickets,

rohan sinha
anish

Email:
Already deployed in some of the world’s largest enterprises, Traversal improves the resilience of mission-critical systems — reducing MTTD and MTTR by up to 90% and supporting services that reach millions globally.

Hey Rohan!

Saw the recent news about the Series A raise - congrats! I'm currently a TPM at Microsoft in our high performance computing division where I serve some of Azure's largest customers like OpenAI on their workflow orchestration. Naturally, I've been following Thread AI's journey and the evolution of the ecosystem as a whole (have also known Sam on your team since college!)

  
From the outside, what's really catching my attention is how having access to a tool like Lemma would significantly make my job easier, e.g. we're running a maintenance update on our supercomputers today and given how finnicky some of the systems are, it'd be a huge win to connect all the monitoring sources with historical maintenance reports and facilitate an automatic triage plan :) 


I've been a part of conversations to jerry-rig this internally for our customers and know how tough it is, so always interested in learning more about what you all are working on. I'd love to buy you a coffee sometime in the coming weeks if you're up for it; would love to trade notes, hear more about your side of the table, and formally intro myself.


Agenda for H100 remdiation meeting:
1. Go over whether we want to suggest 1/3 at a time or 1 cluster at a time
2. How we're communitating the schedule with OpenAI
	1. I run a weekly sync w OpenAI on maintenance needs for Inglewood lite, that is, 11,27,28
	2. Traditionally what we've done is present our timeline
	3. Make sure we're aligned on the plan
July 21

Raj Kamath - to confirm time, duration, impact to cluster

phx11 was not able to meet sla for 2-3 months
most heavily expected by thermal issues
all clusters will go thru same thing
done in a way thats done 
ideally do one for fleet at scale

Option A) 
all nodes will go to OFR, 
for IB maintenance we want to take cluster down
apply A option for all the clusters 
go back to training 
replace the update

OFR -> go back to production -> details
Process needs to be finalized

Optino B) 
Take SW FW update, and then start updating on failure
![[Pasted image 20250707130024.png]]

Goal: apply this process flow that Boris has outlined to the clusters
PHX11, 61, 27

Action: 
