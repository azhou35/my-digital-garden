---
title: "evals"
---

  
The space of possible inputs for a modern model is impossible to check exhaustively, so evals sample incompletely from a large distribution. Evals help develop a theory of what models are doing under the hood (e.g., they're doing linearized subgraph matching, or they're searching over a continuous space of implicit Turing machines), and we use evals to test whether they fail or succeed at the tasks implied by the theory. We might construe human intelligence as a difference in general horsepower, whereas the difference between models lies in the fact that bigger models have a larger representation of ever-sparser features and a longer tail of things that might arise. If you scale up a DB and keep adding more knowledge / program templates to it, it might become more adept at tackling tasks, but general intelligence is the ability to approach any problem / skill and master it with limited data. Generality isn't specificity scaled up, but the ability to apply the mind to anything, even arbitrary things, which requires some fundamental ability to adapt.  
  
If you look at LLMs closely, they're not really synthesizing new programs on the fly to solve tasks they're faced with, but instead reapplying things they've stored in memory. LLMs can solve a Caesar cipher (e.g., transposing letters to code a message) because they've basically memorized it. Interestingly, they can solve the cipher for common numbers found on the Internet (e.g., 3, 5), but if you try to run the same cipher with an arbitrary cipher, the LLM will fail. It doesn't encode the generalized form of the algorithm but only specific cases.  
  
We measure the performance of LLMs on MMLU / other benchmarks, but what's missing from the benchmarks currently in use? What aspects of human cognition do they not measure adequately? LLMs don't have episodic memory in the same way that humans have working memory (for things that have occurred quite recently) or cortical memory (things stored in the cortex). But there's also episodic memory in the hippocampus for learning specific things rapidly (e.g., if you remember some of the things I say to you tomorrow), and we can try to make context windows (sort of akin to working memory) longer to compensate. Episodic memory is very much related to sample efficiency, and LLMs still have a certain kind of sample efficiency (when something is in their context window, that biases the distribution to behave in a different way). Models can learn things immediately when they're in the context window, and then they have a longer process where they train the base model.