---
title: "2026-01-09"
---

1 pod a week per day 
for 8 hrs 

make sur eu send notifs to phx11 
27
28
61
62

update
LVL05PrdGPC01
aligned 

**`c245 c199 c104 c113 c205 c159 c213 c201a / c201`**

go run ~/code/openai/api/x/simont/maintenances/maint_impact.go --clusters c176 -ip c245 c199 c104 c113 c205 c159 c213 c201 -e table

go run ~/code/openai/api/x/simont/maintenances/maint_impact.go --clusters c176 c245 c199 c104 c113 c205 c159 c213 c201 -ip -e table

go run ~/code/openai/api/x/simont/maintenances/maint_impact.go --clusters c176 -ip 15 -e slack

go run ~/code/openai/api/x/simont/maintenances/maint_impact.go --clusters c199 c104 c113 c205 c159 c213  c201 -ip 15 -e table

the process is likely similar,  if things go wrong my team isnt as aware of their stack is what takes the most time

[2:24](https://openai.slack.com/archives/D0A6T1714TV/p1767997458524869)

e.g. right now c258 nodes arent coming up so we have to debug and figure out whats wrong

[2:25](https://openai.slack.com/archives/D0A6T1714TV/p1767997521152199)

scaledown is the same as any other research cluster we can handle with the rest. we can also hit the azure buttons to scale back up and if it works well, great. but i dont want our team to have final responsibility of return to service for the frontiers clusters

[2:25](https://openai.slack.com/archives/D0A6T1714TV/p1767997543084329)

e.g. if we see its not working can we call upon them to look and finish

Annie Zhou  [2:26 PM](https://openai.slack.com/archives/D0A6T1714TV/p1767997567595059)  

this is a fair scope. i can loop fleet in for the azure work and i will check on them on the health recovery part

[2:26](https://openai.slack.com/archives/D0A6T1714TV/p1767997583838059)

in that case it may even fall more on the hardware health dri who probably will have the same feedback

[2:28](https://openai.slack.com/archives/D0A6T1714TV/p1767997699183199)

thanks alex will come back with more deets - phx28 can also b pushed to 8am

how much help do they need? sounds like they want us to do it for them? what i am worried about is frontiers runs weird setups and I wouldnt want us to spend time debugging it upon scaleup esp if they have different time/pressure constraints than we do (edited)