---
title: "2026-01-21"
---

```markdown
date: 2026-01-21
title: 2026-01-21 todo
tags: 
   - productivity 
```
### P0 
- [x] Finish writeup 
- [x] Prep maintenance flow
- [ ] Fix maintenance ingest
- [x] Catch up on messages
	- [x] CW 
	- [x] GCP
### P1
- [x] Send doc to Zielenski 
- [x] Prep news about Titanium Cluster
### P2
- [x] Must do: 
	- [ ] Cordon next set of nodes
- [ ] Y



## HWH On-Call - 2026-01-21

1. Standardizing APIs
	1. What should our response formats and requests look like for APIs that we need across all CSPs.
	2. What are the right scale-up domains to think through
	3. What operations do we need?
	4. Single-Node operations
	5. Whole-Rack operations
	6. What other abstractions do we need?
	7. What happens if we sign another provider tomorrow? What do they need to build for us and close gaps on instead of us just saying "okay we will turn it on?"


Operational Friction with Azure
1. Coordination & DRI Model

DRI ownership was unclear, with 3+ parallel chats (scheduling, execution, recovery) running simultaneously.

This caused coordination overhead and missed handoffs, especially when issues crossed phases.

Scope drift occurred mid-day, with misalignment on which clusters were actively in scope.

2. Progress Tracking Became Unwieldy

We were effectively juggling three different operational tracks at once:

Cluster execution

VMSS-level debugging

OFR node recovery

There was no single view that cleanly reflected where a cluster actually was across those tracks.

VMSS & Capacity Failure Modes
3. VMSS Blast Radius Across Clusters

PHX27 takedown cascaded into PHX28, because a single VMSS maps to nodes across multiple clusters.

As a result, PHX28 remained down for ~3 days, even though it was not the original target.

This cross-cluster VMSS coupling significantly increased blast radius.

4. Capacity vs. CT Pinning Confusion

Unexpected VMs appeared mid-update on Azure’s side.

Azure attributed this to our autoscaler, but our telemetry showed 0 active VMs.

Root causes:

One incident was our fault (manual intervention during a specific day).

Other incidents were caused by data lag between systems.

Azure repeatedly asked us to turn autoscalers off, which made recovery worse:

With autoscalers on, nodes could trickle back as Azure released capacity.

With autoscalers off, we saw:

Large, bursty VMSS scale-ups

Higher allocation failure rates

Slower container pulls due to over-scaling

~1–2 additional hours per operation in recovery and babysitting

The first few days were the worst, when we were manually guessing scale targets.

Once we understood HEN limits, operations stabilized, aside from a few edge cases.

Recovery & Tail Risk
5. Lag Between “Capacity Returned” and “Service Restored”

After Azure returns capacity, there is an unpredictable lag before the cluster is actually usable (Applied or Research).

Tail latency is highly variable and difficult to reason about.

6. VMSS Scale-Up Debugging Required

Post-maintenance recovery frequently required manual VMSS scale-up debugging.

OFR nodes introduced additional complexity:

e.g., Lutetium IB fabric issues

e.g., Iron-D repave required for cache recovery

actionable 
- go research clusters
- research cluster page 
	- its from incident io 
	- stop using 
	- move research status to go/next
	- can request sev status 
	- mcp / codex 
		- they dont have access to api calls
	- what does a researcher vs agent do 
		- we want agent to go to the page 
	- our compute-support will talk to codex and will talk to plumbing
source of truth 
turnups

starling 
go/bri 
go/next


1) theres a data model thats missing
2) how to automate vmss scale down 

snowflake data -> alerts


maintenance controller 
go/brix to surface to useres


NDB needs to have planned maintenance schema


Tool:
- Spin up a codex to scale up / down

make sure LLM can work on 

### Sam notes
difference:
- i have bigger fleet - but less stringent acceptance requirements / sla
- frontiers has fewer clusters - but higher stringenc y
our alignment is within:
- Maintenance/repaves 
	- getting argon to a usable state, derisking runs, acceptance testing
	- tight coordination 
	- how have u been pushing CSP
	- 
- health/stability
	- stability / NIS / health
- SSOT 
questions:
- “What are the 2–3 failure modes you’re most worried about for Argon / Frontier over the next quarter?”
- “When things conflict—run protection vs infra work—what’s your default tie-breaker?”



how does safety systems training works
- run derisks - test getting safety training data into models
- handoffs for derrisk

GPU compute constraints

timelines 


### Kenny
- peter talks thru scheduling problems 
	- pending pods
- every engineeer says this is wwhat we faced
	- this is what we fixed along the way
- this is what is long tail
- -> is there anything we need to prioritize today 
- how often does this happen
	- data on this 
	- we need to work on this tooling
	- bc the next month we have this many maintenance ops
- create the urgency 
- here's whwat we accomplished
- heres the problems we ran into this
- to wrap everything up this is the long tail problem
- agree internally on what the biggest problems where
	- work w Evan Jane Jack


for https://docs.google.com/document/d/15uzD4rQNPwt83UmFgPFMCMN-sQRESALmR4X4ap7nosU/edit?tab=t.0
- scale down / scale up 
- whatdo we need from csps
- we can script things and trigger apis
- where is go/next sit on this 

go/next
- as a hardware tpm i want to do these things

Definition of success for:
- e.g. % of clusters over SLA of [x]%.
- e.g.  % alignment - all clusters within X version of each other across firmware, k8s, cni, etc
- e.g. X % of human time spent on updates
- e.g. <= 1 SEV3 per month caused by updates

putting it somewhere

we're doing a lot of bottoms up work 
1) that strategy 
2) add more definitions
3) cluster decoms
	1) tear downs 
	2) unblock capacity 

this part is overwhelming to researchers
- they dont care why their job crashes
	- they just care about give us reliable job
- this ui is more for hardware health 
- keep it as high level and then 
- badges 
trying to understand how hardware helath team
	full process flow for to understand how this works

go/next is rlly good at surfacing status
-> how do you push this towards action 
feature provides value 
value has to be actionable 
ui is better for real estate 
and less error prone state

high level: 
trigger progress and feedback 
enabling notification thru the same pipeline
thru apollo:
- push it back for them
	- one of the nodes for the vendors to redo validation 
- reboot 
operational functionality 
cli is very convenient 
- its very convenient for ppl is not convenient
Admin
- big planning item for ACL controls 
- diff access controls 

trend:
- new ppl have harder way of planning and joining
architecture:
- ndb is providing for APIs
- allow for diff clients to consume that well 
maintenance calendar:
- P1 
	- cluster status 
who uses go/next
- mostly scaling folks
- researchers who are working on training clusters
ndb can be source of truth 
- status and data
- brix has good integration 