### thought dump
- listened to dialectic's episode with molly mielke 
- [[moley mielke x dialectic]]
### HWH Ops Call
1. lithium capcity slowdown
2. B200tooold for c253 c349
3. Validation crashing on A100s
4. Azure UA in applied
5. Azure UA in applied 
	1. MSFT asked about using GHR instead of deallocating - updated applied kube sync
6. IB0 switches going down (beluga/vanadium)
	1. Running too close to SLA for these clusters
7. fleet health 
	1. f24 looks sad - due to jumpbox
	2. tiger went bad

sreekanths opinion
renaud 
saagar


tooling to reconcile work

node convergence 
vs cluster convergence

policy is internal 
- what do we care about
- assume every computer is built the same
- assumption that 
- signal for nfz 
internal policy 
- what do we actually need to do 
is cluster convergence 
node level convergence

push a signal to them rather than receive back to them 

go to brixy and go to the workloads on the clusters


### Questions about repave:
1. What does the TPM need to do before hand 
2. What are our success metrics


my requests:
Fully automated maintenance/transfer process via temporal. 
We're working on in-place maintenance, I think it would be a minor extension to make it functional for xfers too
Our tools make it go faster than normal operations
Eng (including all of us) is mostly hands off outside of building control planes

Clear visibility into progress / recovery (cc @cain)A pony (sorry I don't have a third yet and lists need 3+)[6:21 AM]

figuring out how quota works (do we just copy it to the new target without modification? Seems simplest) would be good too

What did we learn from titanium that 


maintenance / daniel / 
apollo - wait until post presentation 


Kerem - apollo tech lead 




Fully deprecate old cluster creation/management styles: infrarepo Research clusters; monolithic Applied engine clusters.
Invest in process + automation for continuous repaving/reimaging/deploying so we stay close to latest stable versions (Kubernetes, GPU drivers, management 

Workstream C: Clusters Capabilities
Lead: Alex Zielenski
Initial members: Oleg Guba Gaston Kleiman Yirui Zhang
Charter: Help Fleet scale our ability to manage the fleet by removing entropy from the fleet and enabling agentic management. Change management to support human or AI initiated changes, staged rollouts, canaries. Digest complexity worked around by other workstreams and build principles, predictability, and shared systems.
Scope
E2E capacity pipeline: asset ingestion ‚Üí cluster creation ‚Üí ZBS ‚Üí Butter v1 ‚Üí validation ‚Üí day-2 operations
Network hubs
OS image build & deployment pipeline
Reliability standards: SLOs, disruption budgets
Infra engineering velocity: infra agent SRE
Compute fungibility
Iterative tooling for image build
H1 2026 outcomes (by June)
Image build iteration is easy:
easy to change OS image and create image for new vendor
build images for all vendors + SKUs
support multiple teams contributing components
automated testing against representative research/applied workloads
Assisted migration of engines off nginx
Nub technology in Applied in time for monolithic burndown
Apollo in Applied nubs (node provisioning + cluster assignment) working on a few clusters
Applied Observability owns & operates Research Observability stack
Proof-of-concept SRE tool responding automatically to SEVs:
ping test, pull logs, find recent deploys, read dashboard
Prototype *SLOs* (weak but met); begin tracking ‚Äúhandoff-to-in-production‚Äù
DNS work
Track drift / ‚Äúrolling update‚Äù hygiene (drivers, workloads, OS images) via automation/tooling/verification
H2 2026 outcomds (end of year)
Unified node provisioning deployed for meaningful portion of fleet
Good SLOs for core infra services
Unified image build across all SKUs/vendors; Research + Applied
Unified node provisioning across all SKUs/vendors; Research + Applied
Apollo for:
node bring-up
node‚Üîcluster assignment
node‚Üîboot image assignment
One consistent way across fleet to:
SSH into any node
update desired VMSS/NodePool image
create a cluster üòÆ
Nginx not installed in clusters
Work themes (examples)
Improve leverage (bootstrap stability; reduce maintenance drag)
Image build upheaval (trampoline boot; cloud-init provisioning; modular kernel/rootfs builds; automation to rotate images)
Applied nub technology (align Applied cluster design with Research; unlock fungibility)
Continuous Karaoke (oncall-to-roadmap feedback loop; qualification automation; agentic operations)
