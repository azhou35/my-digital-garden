---
title: "Ben Thompson on O3"
---

> There has been a lot of talk about the importance of scale in terms of LLM performance; for auto-regressive LLMs that has meant training scale. The more parameters you have, the larger the infrastructure you need, but the payoff is greater accuracy because the model is incorporating that much more information. That certainly still applies to `o1`, as the chart on the left indicates.
> 
> [![o1 scales with both training and inference compute](https://i0.wp.com/stratechery.com/wp-content/uploads/2024/09/o1-2.png?resize=1280%2C719&ssl=1)](https://openai.com/index/learning-to-reason-with-llms/)
> 
> It’s the chart on the right that is the bigger deal: `o1` gets more accurate the more time it spends on compute at inference time. This makes sense intuitively given what I laid out above: the more time spent on compute the more time `o1` can spend spinning up multiple chains-of-thought, checking its answers, and iterating through different approaches and solutions.
> 
> It’s also a big departure from how we have thought about LLMs to date: one of the “benefits” of auto-regressive LLMs is that you’re only generating one answer in a serial manner. Yes, you can get that answer faster with beefier hardware, but that is another way of saying that the pay-off from more inference compute is getting the answer faster; the accuracy of the answer is a function of the underlying model, not the amount of compute brought to bear. Another way to think about it is that the more important question for inference is how much memory is available; the more memory there is, the larger the model, and therefore, the greater amount of accuracy.
> 
> In this `o1` represents a new inference paradigm: yes, you need memory to load the model, but given the same model, answer quality does improve with more compute. The way that I am thinking about it is that more compute is kind of like having more branch predictors, which mean more registers, which require more cache, etc.; this isn’t a perfect analogy, but it is interesting to think about inference compute as being a sort of dynamic memory architecture for LLMs that lets them explore latent space for the best answer.