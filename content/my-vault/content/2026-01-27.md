---
title: "2026-01-27"
---

## 3P Platform Convergence

### Saagar
- Sources of Truth for Fleet Data 

when we do a maintenance its rlly hard to get our capacity back 
build out a owrkflow
toggle the right states 

if we kept the nodes in the same cluster
since we move them around 
when a node moves between cluster
our validation flow has logic where:
- if it's a prev healthy node re-entering a cluster we will fast-track it 
- if it's completely new piece of hardware via the slow path (12 passes)
- all of these is scoped to a cluster 

argon - repaving fully 

pre-maintennace handshake
- automate our maintenance workflow 
- schedule it 
	- one week in advance and it creates ppl
	- quarantines a cluster
	- drains all the nodes 
	- waits for them to come back
	- reverse the whole process
	- the faster we get this 
Workflow
- automatable workflow
heres a place that scheduling can intake
progressive crd updates
node-level maintenance taint 


## HWH Weekly 
- Support from Caro
- Sreekanth 
	- any where we can make life more reasonable 
- Azure GB300
	- Known issues
		- IB XDR broken {NV} 
		- Hypervisor bugs {azure}
		- IMEX wasn't running {us/clusters}
			- results in the names in the rack - uses host names - that needs to be resolved
	- nice-to-have of a 'canary node' to validate
		- config
		- validations
- how do we verify a node is ready
	- node goes thru validation and gets out 
	- check all the things we care about

- NDB load

Post_OFR Validation Dashboard
- look at validation nodes today
- Re-ban rate by vendor
	- With OCI 
		- 30% 
	- CW direct/CW w GCP 
		- 1% 
	- BY SKU 
		- GB300 
			- 15%
	- reban rates by cluster
		- oci is the highest reban rate 
- threshold simulators
	- if we were to reban nodes back in 
	- we dont require any dcdiag to pass
	- if we reduc eof 10
	- of the nodes im additionally letting in what their collective 
- AI: check out this study for the research clusters we looked at 
- a lot of our bans are workload caused
- stakeholder:
	- how much value is there by run 10 vs run 9 

flag
- frontiers vs fleet clusters
- feel free to segment
- 1 rlly big ui
- segment the ownership 
- agent.md 
	- a lot of things set up in place
	- when u run a task against a code base
- quetstion
	- make sure that its highly iterable via codex


me / mark / simon talk about it

say in fleet team 
this si what makes sense
- i need some input from these people i may be setting up time with u 
- if u need consensus on something put it in public channel 

biweekly meeting 
- Sreekanth biweekly meeting 


#### Clusters <> HWH 
- if we reboot a node how do we track which nodes come back
- all these issues are in applied

metrics
- measure % of nodes that are rebooted

research side when we try to reboot:
- get a metric of whether the node actually went offline 
- did the node come back

Zielenski:
- standardize all nodes to use boot esrver

flags to scheduling:
this is a reservation oncept
problem - expect tooling to work


global scheduling
- ppl are handed allocations
	- pools are ultimately related to ibkey 
	- if someone requests 40 nodes of compute
- if we move 10 nodes at a time


## Thoughts
- file > app 
- convo w coworkers about socializing 
- closed loop systems
- talking about making hardware toys for kids
	- gaming/hardware as an avenue for learning
	- https://www.dex.camera/?pb=0 
[[content/how can gaming be an avenue for learning?]]
- how is ai going to support
	- student/child development
	- aging populations
	- the maintainers of the world
	- the laborers of the world