---
title: "gb200"
---

- superchip packs two b200 blackwell gpus + grace gpu on single board
- ~208B transistors
- nvlink meory domain 
- organizes GB200 Nvl72 racks into 18 compute tray s + 9 NVSwitch trays
	- each compute tray holds 2 bianca boards (4 gpus + 2 cpus per tray) for 72 gpus and 36 cpus per rack
	- trays carry dual 28.8TB/s fifth gen NVL switch chips to interconnect all 72 GPUS
	- =? single 72 gpu nvlink domain 
	- any gb200 gpu can access any other memory at 1.8 TB/s
		- what is the significance
- n total, an NVL72 rack offers ~6,480 TFLOPS FP32 across 72 GPUs and ~13.5 TB of HBM3e memory (576 TB/s aggregate)[amax.com](https://www.amax.com/solutions/nvidia-gb200-nvl72-liquid-to-air-rack-solutions/#:~:text=Per%20Rack%20Memory%20and%20Bandwidth).

NVL/NVS are to counteract communication bottlenecks 

![[Pasted image 20250831162102.png]]**A GB200 NVL72 rack is ~120 kW at full load, which is **far above standard air-cooled racks** (~10–20 kW). Problems that can arise without proper sequencing:

- **Breaker trips / overloads:** Turning on all PSUs simultaneously can exceed circuit ratings.
    
- **Cooling dependency:** GPUs need liquid cooling to prevent thermal shutdown.
    
- **NVLink / NVSwitch initialization:** NVSwitch ASICs may fail to sync if powered on before stable power and cooling.
    
- **Automation scripts:** Anvil/NSD may mark nodes unhealthy if nodes start before the environment is ready.**

PDU = device that takes high voltage feed from dc -> into multiple servers/racks
single rack draws **120 kW** so u have multiple PDUs feeding same rack to distribute load evenly
compared to 40kW/rack for H100
breaker
electrical safety device that interrupts power if current exceds a safe limit

PS = health and status of a servers power supply

next form factor is the **GB200 NVL36 * 2** which is two racks side by side interconnected together - most GB200s use this factor
![[Pasted image 20250831185222.png]]

most power draw is from the two Biana boards and 8 fans
#### Networking
1. Four types in GB200
	1. front end networking - ethernet to connect to storage and DC management
	2. Backend - infiniband/ethernet used for GPU-GPU across racks
	3. out of band - BMC, PDU, CDUs, fan/temp/power monitoring
2. NVL Interconnect
	1. NVL72 - 1 hop for any gpu in a rack
3. backend nic/switches: CX7-> CX8 upgrade, multiple switch variants
4. cabling - dac / acc copper 


### Compute