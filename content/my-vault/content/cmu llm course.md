---
title: "cmu llm course"
---


[Syllabus - Large Language Models: Methods and Applications / Fall 2024](https://2024.cmu-llms.org/syllabus/)

Large Language Models Methods and Applications (11-667) is a graduate-level course that aims to provide a holistic view of the current state of large language models. The first half of this course starts with the basic of language models, including network architectures, training, inference, and evaluation. Then it discusses the interpretation (or attempts of), alignments, and emergent capabilities of large language models, followed by its popular applications in language tasks and new utilizations beyond texts. In the second half, this course first presents the techniques of scaling up language model pretraining and recent approaches in making the pretraining of large models and their deployment more efficient. It then discusses various concerns surrounding the deployment of large language models and wraps up with the challenges and frontiers of LLM developments.
## Learning Goals

Students who successfully complete this course will be able to:

- Compare and contrast different models in the LLM ecosystem in order to determine the best model for any given task.
- Implement and train a neural language model from scratch in Pytorch.
- Utilize open-source libraries to finetune and do inference with popular pre-trained language models.
- Understand how to apply LLMs to a variety of downstream applications, and how decisions made during pre-training affect suitability for these tasks.
- Read and comprehend recent, academic papers on LLMs and have knowledge of the common terms used in them (alignment, scaling laws, RLHF, prompt engineering, instruction tuning, etc.).
- Design new methodologies to leverage existing large scale language models in novel ways.
- 