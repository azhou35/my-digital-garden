---
title: "1-16-2026"
---

### CW Maintenance
- Slight risk that we dont cordon nodes under the racks
- Tightened up powering down nodes in morning
- nearly identical systems 
- flagged in teams where a couple where they were not correct
- 42K6J84, F4L6J84 and 5Z52NB4
- capacity return for cdu maintenance
	- upgrade
	- GPU FW 1.3 
- three diff versions of FW 

meet first week of feb 
- document ready w FW
- agreement on IB FW 
	- how much is left on the FW 
	- depending on errors on fabric it becomes better option to upgrade the fabric rather than remediate

FW 1.3 
- - **1.1** issues: GPUs fall off the bus 
		- can be resolved w 1.3
		- homogeneous within the rack 
		- we may start asking for the rack to be returned so we can upgrade the rack 
- metrics for GPU falling off the bus  
- all sites
- get you a better idea of how many racks / tickets do we have
	- we cant get the FW version for baremetal 
	- only for GB200 / GPU falling off the bus / snyder / Phoenix 
- snapshot 
- is it a risk
- open request - would u want 
	- EU node / node A 
	- GPU fell off the bus 
	- EU NORTH A / FW 1.1 North B
- we want to keep versions homogenous 
- on demand is there a way to query what versions a CW is on 
- in a location where u have 2 versions or FW upgrade

karen 
- 1 row per day 
- basically it's a risk that we can recover nodes by the day
- 

- IB fabric cannot be updated without nodes going down 
	- your tooling does not support 
- if ur looking at upgrading all ur ib switches - u want a short peiriod of time of all of them together
	- while ur upgrading ur fabric - in theory gpu nodes arent impacted
	- but its set up such that if u take down switches u take down nodes
- gio/douglas tha we need to have some sort of process
- standards wouldnt jut be for me or u but for everyone


thoughts: what are the right signals to expose to researches 