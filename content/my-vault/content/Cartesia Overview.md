---
title: "Cartesia Overview"
---

LLM / Transformes
- As # of words incr # of computatioal steps grow quadratically

SSMs:
* Fixed-size recurrent state - state needs to be larger 
[Tri Dao on X: "Empirically, our most important results are on language modeling, an area where previous SSMs targeted (e.g. H3, the predecessor of Mamba https://t.co/40YocUCLpc) but still fell short of Transformers. Thus far, no model has actually matched a well-tuned modern Transformer... 7/" / X](https://x.com/tri_dao/status/1731728611995181302)
is this Mamba related to TriDao's?
### Products
[Research | Cartesia](https://cartesia.ai/research)

[Understanding State Space Models (SSMs) like LSSL, H3, S4 and Mamba](https://tinkerd.net/blog/machine-learning/state-space-models/) 
###  Questions to Ask:
