---
title: "fireworks ai"
---

### Notes
level of abstraction:
- api-only experience (vs offering level of customzized knobs)
- similar to developer exp provided by chatgpt 
- doesnt allow users to customize low level things like which gpu

headwinds: 
- tam expansion - larger # of companies building solutions for smaller markets 
- product expansion - expand and own entire traditional backend abstraction platform like vercel/google firebase before platforms role out their own ai solutions

tailwinds:
- over-abstraction - some customers want to control the type of gpu, or other levels
ai inference vendors become *more full stack* and 1 stop shop for developers. as developer ergo and performance converge, key wins comes from distribution in sales and marketing.

src: [A Deep Dive on AI Inference Startups - by Kevin Zhang](https://eastwind.substack.com/p/a-deep-dive-on-ai-inference-startups)

### Why Fireworks
Working within Microsoft's AI infrastructure team, I witnessed a fundamental disconnect: brilliant researchers spending more time wrestling with infrastructure than scientific progress. This pattern convinced me that AI's next inflection point isn't about better models, but about making production-grade deployment easier. 

The real opportunity lies in helping developers deploy and scale AI systems with ease. That's where Fireworks stands out: as a developer-first platform it prioritizes usability just as much as performance, with a clear vision for building compound systems that reshape how GenAI applications are developed. 

From developing copilots for customers that automate workflows to orchestrating supercomputers for OpenAI, I've seen first-hand the complexity of building and maintaining GPU infrastructure. Fireworks is attacking this complexity head-on. 

From my chat with Ray, it’s clear the PM role is both technically rigorous and customer-obsessed. It's a chance to shape the roadmap and solve tough constraints in latency, cost, and safety. In a market where great infra isn’t enough — it needs to be usable, scalable, and opinionated — this role feels like the perfect fit for the problems I want to help solve.


1. **Why Fireworks?**
    
    - Want to move closer to the product and modeling edge of LLMs.
        
    - Fireworks is working on inference — a bottleneck for real-time AI products. You want to help unlock new LLM-native use cases.
        
    - You’re excited about joining a high-agency, small team operating at frontier scale.
        
2. **Why Now?**
    
    - You've built muscle in orchestration and reliability at cloud scale.
        
    - You’re seeking a role where you can ship faster, closer to model + product iteration cycles.

### Recruiter Call
> “ I’'ve been a TPM at Microsoft AI Infra where I'm driving supercomputing infra for OpenAI’s training and inference workloads — managing reliability, orchestration, and software-driven ops for 5K+ H100/H200 GPUs. Also worked across finance and healthcare enterprises on their workload scheduler strategy. That’s given me deep exposure to how AI infrastructure operates at scale, but also how brittle and complex it is today. I've always found my strength in bridging the technical and the nontechnical, and I love working directly with the customer,  from building dashboards and automations to shipping multi-agent Copilot architectures based on genomics use cases.

I've always been startup curious - worked in small startup in legal tech where i built out contract redlining features. I was doing research on other inference platforms for Contrary VC when I came across Fireworks.

I've been excited about Fireworks for a while - I reached out to Ray when researching inference platforms and had a great convo , who had a similar move from Google Ai to Fireworks. I believe the real bottleneck for genAI to reach production is the speed of inference, and I have high conviction that Fireworks is the place moving this forward. . I want to be closer to the customer and model layer, where the feedback loop is tighter and product-building is faster.

The forward deployed PM role feels like a perfect intersection of technical depth and customer empathy — helping developers solve novel problems using state-of-the-art infra.. 
> 
> I’m excited about Fireworks because you’re taking an ambitious and performance-driven approach to inference — an area I see as the bottleneck for generative AI to reach real-time and production maturity. I want to be closer to the customer and model layer, where the feedback loop is tighter and product-building is faster.
> 
> My ideal role sits at the intersection of technical depth and customer empathy — helping developers solve novel problems using state-of-the-art infra. This Forward Deployed PM role feels like exactly that.”

### Why now? Why Fireworks?
> Over the past year, I’ve been leading orchestration and operational excellence for OpenAI’s training workloads on Azure’s AI supercomputing infrastructure. That’s given me a deep appreciation for what it takes to scale and maintain critical AI systems—but I’m now looking for a faster build-measure-learn cycle, where I can ship more frequently and work at the boundary of customer problems and model behavior. Fireworks feels like the ideal place to do that: fast, technical, product-led, and deeply customer-oriented.
> **There's three things I look for in a new role: team fit, domain fit, and growth.**
> **Fireworks is elading the charge, with a killer team and PMF** 


#### Questions
- “How do you see Fireworks differentiating itself long-term against other inference infra players like Together, Modal, or Groq?”
- “What are the most common use cases you’re seeing in production today? What feels like the next wave?”
- “What makes someone successful in this Forward Deployed PM role?”
- What’s the current team size and growth trajectory for FDPMs or PMs generally?
- What is the current state of growht for Fireworks - ive heard you guys are raising rounds, how does a FPM fit into that 

> how each enterprise defines safety is dif
- daily interaction w customers
	- responsible for engagement
- strategic roadmapping
- in the weeds of the technical
	- set clea constraints
	- curate marking datasets
	- deeply understand whats

## Recruiter notes


cold reached out 
being the foudning pm 
even tho series b 
nice to have a startup 
has strong technical bench
strong pmf
regardless of how many models come out
talent
priority is to scale out
make sure that any customers take 

overlap
interview is bound to reflect that
3 rounds
1st round 
- foundational pm skills
- chat w pms 
- ur pm specific experience
- hw u work w customers
- how u drive conversations to gather requiremnts
- translate to align w business offerings
- prioritze requests
- success 
2nd take home
- customer solutioning 
- that will be the foundation of 2nd 
- how do u define and propose customer solutions
- how u onboard new customes
- assessing ur assignment
- assumptions, tradeoffs, criteria to expand

2nd round
- cross functional partnerships
- gpm of partnerships
	- sales team
	- understanding how ud partner w customer strat
	- how to break down complex customer needs
- technical conversation/experience
	- llms/inference
	- optimization levels
	- how r they used 
	- why r they used
	- scheduled w engineeing leadership

final interview
- co founders - share the vision - how theyve grown 
- making sure that u understand the high and lows

pref is new york 
equity is a big driver
exponentially grow

multiple headcounts - multiple aspects
genai startups vs entreprise
branding and marketing
make sure that it aligns

3 pms - having that pm partnership is so critical 
they do a great job on how the engineering offering ties with customer
having them early on in the convo
drive convo from intiial convo 
a lot of interest
nuanced views
grown a lot - in startup mode

ai development
the smaller the company, the quicker u can react
u can wear so many hats 

pls keep me posted
